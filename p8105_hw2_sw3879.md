p8105_hw2_sw3879
================
Siqing Wang
2023-09-27

Importing libraries

``` r
library(tidyverse)
library(dplyr)
```

## Problem 1

Read `pols-month.csv` into a dataframe and clean up data by adding the
president variable, updating month names, and removing unnessary
variables

``` r
pols_month_df = 
  read_csv("data/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(mon, into = c("year", "month", "day"), sep = "-") |> 
  mutate(
    year = as.numeric(year),
    month = month.name[as.numeric(month)],
    president = ifelse(
      prez_gop > prez_dem, "gop", "dem"
    )
  ) |> 
  select(-day, -prez_gop, -prez_dem)
```

Cleaning data in `snp.csv`, similar to the above procedure

``` r
snp_df = 
  read_csv("data/snp.csv") |> 
  janitor::clean_names() |> 
  mutate(
    date = format(as.Date(date, format = "%m/%d/%y"), "%Y/%m/%d")
  ) |> 
  separate(date, into = c("year", "month", "day"), sep = "/") |> 
  mutate(
    month = month.name[as.numeric(month)],
    year = as.numeric(year),
    year = ifelse(year > 2023, year - 100, year)
  ) |> 
  select(-day)
```

Pivoting `unemployment.csv` to long format

``` r
unemployment_df = 
  read_csv("data/unemployment.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "unemployment"
  ) |> 
  mutate(
    year = as.numeric(year),
    month = month.name[match(month, tolower(month.abb))]
  )
```

Merging the 3 datasets

``` r
result_df = left_join(pols_month_df,snp_df, by = c("year", "month")) |> 
  left_join(unemployment_df, by = c("year", "month"))
```

The `pols-month` dataset has 822 observations and 9 variables after
cleaning. This dataset provides information on party affiliation and
president information on different days throughout 1947 to 2015.

The `snp` dataset has 787 observations and 3 variables after cleaning,
with data available from 1950 to 2015.

The `employment` dataset has 816 observations and 3 variables after
cleaning, with data available from 1948 to 2015.

The combined dataset has 822 observations and 11 variables after
cleaning, from the `NA`s in `close` and `unemployment` variable, we know
some data are missing during those time.

## Problem 2

Read in the `Mr. Trash Wheel` sheet, clean names and remove unnecessary
rows, add trash wheel identifying name, and recalculating homes powered

``` r
trash_wheel_df = 
  readxl::read_excel("data/202309 Trash Wheel Collection Data.xlsx", 
                     sheet = "Mr. Trash Wheel", skip = 1) |> 
   janitor::clean_names() |>
  drop_na(dumpster) |> 
  select(-x15, -x16) |> 
  mutate(
    homes_powered = (weight_tons*500)/30,
    trash_wheel_name = "Mr. Trash Wheel",
    year = as.numeric(year)
  )
```

Read in professor trash wheel and gwynnda, perform similar cleaning as
above

``` r
gwynnda_trash_wheel_df = 
  readxl::read_excel("data/202309 Trash Wheel Collection Data.xlsx", 
                     sheet = "Gwynnda Trash Wheel", skip = 1) |> 
   janitor::clean_names() |>
  drop_na(dumpster) |> 
  mutate(
    homes_powered = (weight_tons*500)/30,
    trash_wheel_name = "Gwynnda Trash Wheel",
    year = as.numeric(year)
  )

prof_trash_wheel_df = 
  readxl::read_excel("data/202309 Trash Wheel Collection Data.xlsx", 
                     sheet = "Professor Trash Wheel", skip = 1) |> 
   janitor::clean_names() |>
  drop_na(dumpster) |> 
  mutate(
    homes_powered = (weight_tons*500)/30,
    trash_wheel_name = "Professor Trash Wheel",
    year = as.numeric(year)
  )
```

Combining three datasets

``` r
trashwheel_master = bind_rows(trash_wheel_df, prof_trash_wheel_df, gwynnda_trash_wheel_df) |> select(trash_wheel_name = "trash_wheel_name", everything())
```

The `Mr trash wheel` sheet has 584 observations and 15 columns.

The `Professor Trash Wheel` sheet has 106 observations and 14 columns.

The `Gwynnda Trash Wheel` sheet has 155 observations and 13 columns.

The combined master datasets for trash wheel has 845 observations and 15
columns.

These datasets record details of trash collected, with key variables
such as dumpster id, date, weight, and litter type such as plastic
bottles, cigarette butts, glass bottles, etc. There are missing data in
`glass_bottles`, `sports balls` and `wrappers` in some column, meaning
that these trash was not collected by those specific dumpsters.

The total weight of trash collected by Professor trash wheel is 216.26
tons.

the total number of cigarette butts collected by Gwynnda in July of 2021
is 16300.

## Problem 3

Reading in `baseline` csv, recode variables, remove ineligible
participants

``` r
baseline = 
  read_csv("data/MCI_baseline.csv", skip = 1) |> 
  janitor::clean_names() |> 
  mutate(
    sex = case_match(
      sex,
      1 ~ "male",
      0 ~ "female"
    ),
    apoe4 = case_match(
      apoe4,
      1 ~ "carrier",
      0 ~ "non-carrier"
    )
  ) |> 
  filter(age_at_onset == "." | age_at_onset > current_age)
```

The important steps in importing this dataset is to recode binary
variables and remove ineligible participants by removing all records
whose onset for MCI is earlier or at the same age at baseline. 4
participants were removed, the demographics information at baseline is
complete.

479 participants were recruited at baseline.

93 participants developed MCI during the study.

The average baseline age is 65.

Proportion of women who are APOE 4 carrier is 30%.

Similarly, clean the `amyloid` set, and pivot from wide to long format

``` r
amyloid = 
  read_csv("data/mci_amyloid.csv", skip = 1) |> 
  janitor::clean_names() |> 
  rename("id" = "study_id") 
```

Pivot `amyloid` from wide to long format

``` r
amyloid_long = amyloid |> pivot_longer(
  baseline:time_8,
  names_to = "visit",
  values_to = "amyloid_ratio"
)
```

The import process is more straightforward, other than cleaning names,
the study_id has to be renamed to match the `baseline` dataset to
proceed with merge. There are 487 participants in the longitudinal
dataset, and MCI is recorded at baseline, time 2, time 4, time 6, and
time 8. There are a significant amount of missing values meaning that
many participants missed visits.

Merge the amyloid dataset with baseline, keeping participants in either
sets

``` r
amyloid_full = 
  full_join(baseline, amyloid, by = c("id"))
```

There are 16 participants in the amyloid set but not in the baseline
set.

There are 8 participants in the baseline set but not in the amyloid set.

Merge the amyloid long format dataset with baseline to keep those in
both datasets

``` r
amyloid_both = 
  inner_join(baseline, amyloid_long, by = c("id"))
```

There are 471 participants that are in both the baseline and
longitudinal visit datasets. There are 2355 rows and 8 columns in the
merged dataset.

Exporting the result dataframe to a csv file

``` r
write.csv(amyloid_both, "data/baseline_amyloid_merged.csv", row.names = FALSE)
```

TO DO ask TA:

1.  Check whether some participants appear in only the baseline or
    amyloid datasets. Should we include participants who are ineligible?

2.  Similarly, import, clean, and tidy the dataset of longitudinally
    observed biomarker value. Should we remove rows with missing value
    in a timepoint?
